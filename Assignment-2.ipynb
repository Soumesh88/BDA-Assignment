{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829cab79-4167-44ab-a855-ecaab38cf84e",
   "metadata": {},
   "source": [
    "# Importing environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f04110-2f10-4f4c-a35b-ef97a73a3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"D:/hadoop/hadoop-3.4.1/bin\"\n",
    "os.environ[\"JAVA_HOME\"] = \"D:/jdk-hotspot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46caf299-4144-4234-9b82-ceb8557ecfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"MONGO_URI\")\n",
    "_set_env(\"BEARER_TOKEN\")\n",
    "_set_env(\"REDDIT_CLIENT_ID\")\n",
    "_set_env(\"REDDIT_CLIENT_SECRET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a05bed-3ded-43a5-bf4d-f0ae4f3af65a",
   "metadata": {},
   "source": [
    "# Importing stocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a74ff96-1b6d-4638-bffc-346723607998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All stock data fetched, combined, stored in MongoDB, and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Stock symbols to fetch\n",
    "symbols = [\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "# MongoDB setup (NoSQL)\n",
    "client = MongoClient(os.environ[\"MONGO_URI\"])\n",
    "db = client[\"stock_market\"]\n",
    "collection = db[\"stock_prices\"]\n",
    "\n",
    "all_data = []  # Accumulator\n",
    "\n",
    "# Fetch and store\n",
    "for symbol in symbols:\n",
    "    ticker = yf.Ticker(symbol)\n",
    "    data = ticker.history(period=\"1y\")  # past 1 year data\n",
    "    data.reset_index(inplace=True)\n",
    "    data[\"symbol\"] = symbol\n",
    "\n",
    "    # Convert datetime to ISO format for Mongo\n",
    "    data[\"Date\"] = data[\"Date\"].apply(lambda x: x.to_pydatetime().isoformat())\n",
    "\n",
    "    # Store in Mongo\n",
    "    records = data.to_dict(orient=\"records\")\n",
    "    collection.insert_many(records)\n",
    "\n",
    "    # Accumulate for CSV\n",
    "    all_data.append(data)\n",
    "\n",
    "# Concatenate all dataframes and export\n",
    "combined_df = pd.concat(all_data)\n",
    "combined_df.to_csv(\"stock_data.csv\", index=False)\n",
    "\n",
    "# HDFS Upload\n",
    "!hdfs dfs -mkdir -p /stock_market_data/\n",
    "!hdfs dfs -put -f stock_data.csv /stock_market_data/\n",
    "\n",
    "print(\"✅ All stock data fetched, combined, stored in MongoDB, and saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8a7b8-d6ca-4f57-b83b-1c24ffb85469",
   "metadata": {},
   "source": [
    "# Importing tweets for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1358fb-ae6b-4653-bcd8-4411b5375b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tweets fetched and stored with sentiment.\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Tweepy v2 Client\n",
    "client = tweepy.Client(bearer_token=os.environ[\"BEARER_TOKEN\"])\n",
    "\n",
    "# MongoDB setup\n",
    "mongo_client = MongoClient(os.environ[\"MONGO_URI\"])\n",
    "db = mongo_client[\"stock_market\"]\n",
    "sentiment_collection = db[\"twitter_sentiment\"]\n",
    "\n",
    "# Define query and fetch tweets\n",
    "query = \"AAPL OR TSLA OR stock market -is:retweet lang:en\"\n",
    "tweets = client.search_recent_tweets(query=query, max_results=50, tweet_fields=[\"created_at\", \"text\"])\n",
    "\n",
    "# Process and store tweets\n",
    "for tweet in tweets.data:\n",
    "    sentiment = TextBlob(tweet.text).sentiment.polarity\n",
    "    doc = {\n",
    "        \"text\": tweet.text,\n",
    "        \"timestamp\": tweet.created_at.isoformat(),\n",
    "        \"polarity\": sentiment\n",
    "    }\n",
    "    sentiment_collection.insert_one(doc)\n",
    "\n",
    "print(\"✅ Tweets fetched and stored with sentiment.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068e61f-cabe-4854-85d2-2354efb19405",
   "metadata": {},
   "source": [
    "# Importing Reddit posts for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f158e224-0183-4c35-8a49-ee8c5872ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soume\\AppData\\Local\\Temp\\ipykernel_23576\\1428774017.py:20: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  \"timestamp\": datetime.utcfromtimestamp(post.created_utc).isoformat(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reddit sentiment stored.\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from pymongo import MongoClient\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "\n",
    "mongo_client = MongoClient(os.environ[\"MONGO_URI\"])\n",
    "db = mongo_client[\"stock_market\"]\n",
    "\n",
    "reddit = praw.Reddit(client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "                     client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "                     user_agent='Stocks')\n",
    "\n",
    "posts = reddit.subreddit('stocks').hot(limit=50)\n",
    "\n",
    "for post in posts:\n",
    "    sentiment = TextBlob(post.title + \" \" + post.selftext).sentiment.polarity\n",
    "    doc = {\n",
    "        \"title\": post.title,\n",
    "        \"content\": post.selftext,\n",
    "        \"timestamp\": datetime.utcfromtimestamp(post.created_utc).isoformat(),\n",
    "        \"sentiment\": sentiment\n",
    "    }\n",
    "    db[\"reddit_sentiment\"].insert_one(doc)\n",
    "\n",
    "print(\"✅ Reddit sentiment stored.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a75e5-786f-4db5-99b1-ad6e8ca4d42f",
   "metadata": {},
   "source": [
    "# Extracting meaningful features using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e144ea-d0b7-48e5-a1f9-a706e18b5274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------------------------------------------------+\n",
      "|Date               |symbol|features                                                    |\n",
      "+-------------------+------+------------------------------------------------------------+\n",
      "|2024-04-15 09:30:00|AAPL  |[-3.6034592590327206,-0.8590518681222201,2.1856737306147562]|\n",
      "|2024-04-16 09:30:00|AAPL  |[-3.5336865656799894,-0.9298950707215315,2.1237477016764656]|\n",
      "|2024-04-17 09:30:00|AAPL  |[-3.553163962284043,-0.8588893233473567,1.4317278472627286] |\n",
      "|2024-04-18 09:30:00|AAPL  |[-3.5421520010728385,-0.7975403328313077,1.2244521861873687]|\n",
      "|2024-04-19 09:30:00|AAPL  |[-3.434311064622725,-1.0143600966961897,1.8434163503131928] |\n",
      "+-------------------+------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, avg\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Feature Engineering\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load stock data\n",
    "df = spark.read.csv(\"stock_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert Date to timestamp and sort\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\")) \\\n",
    "       .orderBy(\"symbol\", \"Date\")\n",
    "\n",
    "# Create rolling average and daily returns\n",
    "windowSpec = Window.partitionBy(\"symbol\").orderBy(\"Date\")\n",
    "\n",
    "df = df.withColumn(\"Prev_Close\", lag(\"Close\").over(windowSpec))\n",
    "df = df.withColumn(\"Daily_Return\", (col(\"Close\") - col(\"Prev_Close\")) / col(\"Prev_Close\"))\n",
    "df = df.withColumn(\"MA_5\", avg(\"Close\").over(windowSpec.rowsBetween(-4, 0)))\n",
    "df = df.dropna()\n",
    "\n",
    "# Feature Vector\n",
    "feature_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Daily_Return\", \"MA_5\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\")\n",
    "\n",
    "# PCA to reduce to 3 principal components\n",
    "pca = PCA(k=3, inputCol=\"features_scaled\", outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "model = pipeline.fit(df)\n",
    "transformed = model.transform(df)\n",
    "\n",
    "transformed.select(\"Date\", \"symbol\", \"features\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6322a212-9e81-4f62-9fe6-2820b28fa5e5",
   "metadata": {},
   "source": [
    "# Applying different ML and statistical models on data and calculating RMSE and R2 score for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80b27f6-b901-4d94-bd1a-3bf4d5655f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RMSE using Isotonic Regression: 105.10832242725765\n",
      "✅ R2 Score using Isotonic Regression: -0.0004212473517037907\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import IsotonicRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "df = transformed.withColumn(\"Next_Close\", lead(\"Close\", 1).over(windowSpec))\n",
    "df = df.dropna()\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "\n",
    "slicer = VectorSlicer(inputCol=\"features\", outputCol=\"feature1\", indices=[0])  # use the first feature\n",
    "sliced_train = slicer.transform(train_data)\n",
    "sliced_test = slicer.transform(test_data)\n",
    "\n",
    "iso = IsotonicRegression(featuresCol=\"feature1\", labelCol=\"Next_Close\")\n",
    "iso_model = iso.fit(sliced_train)\n",
    "iso_predictions = iso_model.transform(sliced_test)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"Next_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Close\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2 = r2_evaluator.evaluate(iso_predictions)\n",
    "rmse = evaluator.evaluate(iso_predictions)\n",
    "print(f\"✅ RMSE using Isotonic Regression: {rmse}\")\n",
    "print(f\"✅ R2 Score using Isotonic Regression: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91969782-ad87-4a36-b759-92ce55860445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RMSE using Decision Tree Regression: 10.469868366388567\n",
      "✅ R2 Score using Decision Tree Regression: 0.990073614426227\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "# Add target column (next day close)\n",
    "df = transformed.withColumn(\"Next_Close\", lead(\"Close\", 1).over(windowSpec))\n",
    "df = df.dropna()\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Choose a regression model (can swap later for comparison)\n",
    "regressor = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"Next_Close\")\n",
    "\n",
    "# Train\n",
    "model = regressor.fit(train_data)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"Next_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Close\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(f\"✅ RMSE using Decision Tree Regression: {rmse}\")\n",
    "print(f\"✅ R2 Score using Decision Tree Regression: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fd6731-f49a-4cad-a29e-955d4e8ac98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RMSE using GBT Regression: 9.367053276512667\n",
      "✅ R2 Score using GBT Regression: 0.9920546198994032\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "# Add target column (next day close)\n",
    "df = transformed.withColumn(\"Next_Close\", lead(\"Close\", 1).over(windowSpec))\n",
    "df = df.dropna()\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Choose a regression model (can swap later for comparison)\n",
    "regressor = GBTRegressor(featuresCol=\"features\", labelCol=\"Next_Close\")\n",
    "\n",
    "# Train\n",
    "model = regressor.fit(train_data)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"Next_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Close\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"✅ RMSE using GBT Regression: {rmse}\")\n",
    "print(f\"✅ R2 Score using GBT Regression: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0bb27e2-9603-4254-b3d8-921a59e0287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RMSE using Linear Regression: 5.839494523771556\n",
      "✅ R2 Score using Linear Regression: 0.996912128574625\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, GBTRegressor, LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "# Add target column (next day close)\n",
    "df = transformed.withColumn(\"Next_Close\", lead(\"Close\", 1).over(windowSpec))\n",
    "df = df.dropna()\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Choose a regression model (can swap later for comparison)\n",
    "regressor = LinearRegression(featuresCol=\"features\", labelCol=\"Next_Close\")\n",
    "\n",
    "# Train\n",
    "model = regressor.fit(train_data)\n",
    "\n",
    "# Predict\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"Next_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "r2_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Next_Close\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"✅ RMSE using Linear Regression: {rmse}\")\n",
    "print(f\"✅ R2 Score using Linear Regression: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450de81-5626-4cb9-b70e-c6c66cdb4420",
   "metadata": {},
   "source": [
    "# Using Linear Regression predictions for visualization and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beeaaba4-5617-439d-8761-b4a8e25e0ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 5.839494523771556\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"Next_Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4787ba8-0775-4e67-a121-7d1f18511537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns (adjust column names as per your schema)\n",
    "output_df = predictions.select(\"Date\", \"symbol\", \"Next_Close\", \"prediction\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_df = output_df.toPandas()\n",
    "\n",
    "# Save to CSV\n",
    "pandas_df.to_csv(\"predicted_stock_prices.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f76aca-0cdd-45e5-98a8-98eac5544631",
   "metadata": {},
   "source": [
    "# Creating Hive table for pattern analysis and performing query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9768306f-2eb3-4a84-848c-f90d0c805f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"stock_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df_cleaned = df.select(\n",
    "    col(\"symbol\"),\n",
    "    col(\"Date\").alias(\"date\"),\n",
    "    col(\"Close\").alias(\"close\"),\n",
    "    col(\"Volume\").alias(\"volume\")\n",
    ")\n",
    "\n",
    "df_cleaned.createOrReplaceTempView(\"stock_data\")\n",
    "\n",
    "#Create a Hive table and insert data into it\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS stock_data_hive (\n",
    "        symbol STRING,\n",
    "        date TIMESTAMP,\n",
    "        close DOUBLE,\n",
    "        volume DOUBLE\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into Hive table\n",
    "df_cleaned.write.mode(\"overwrite\").insertInto(\"stock_data_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b53826-e3c4-41e4-9da1-b5576ad10ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|symbol|         avg_close|\n",
      "+------+------------------+\n",
      "|  MSFT|417.72814029526427|\n",
      "|  TSLA|  266.694980028616|\n",
      "|  AAPL|219.79621139465576|\n",
      "| GOOGL|172.51824476994366|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT symbol, AVG(close) as avg_close\n",
    "FROM stock_data_hive\n",
    "GROUP BY symbol\n",
    "ORDER BY avg_close DESC\n",
    "\"\"\"\n",
    "result = spark.sql(query)\n",
    "result.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
